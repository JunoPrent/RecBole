{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import json\n",
    "import math\n",
    "from random import sample\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribute(df, idx, attribute):\n",
    "    return df.loc[idx][attribute]\n",
    "\n",
    "def get_rating(df, user_idx, item_idx):\n",
    "    return df.loc[user_idx].loc[item_idx][\"rating:float\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remapped_losses(model, dataset, l_r, e_s, fair):\n",
    "    # Loads the remappings made by the model during its training phase\n",
    "    with open(f\"remappings/{dataset}.json\") as f:\n",
    "        remappings = json.load(f)\n",
    "\n",
    "    # Removes unneeded padding\n",
    "    del remappings[\"user_id\"][\"[PAD]\"]\n",
    "    del remappings[\"item_id\"][\"[PAD]\"]\n",
    "\n",
    "    # remappings is of the form {new_id: old_id} and can thus reverse the new ids\n",
    "    # Items without any rating were never assigned a new ID and thus are still the\n",
    "    # same without being reversed.\n",
    "    try:\n",
    "        remappings = {alias: {int(v): int(k) for k, v in remappings[alias].items()} for alias in remappings.keys()}\n",
    "    except ValueError:\n",
    "        remappings = {alias: {int(v): k for k, v in remappings[alias].items()} for alias in remappings.keys()}\n",
    "\n",
    "    loss_info = pd.read_csv(f\"training_losses/{model}/{dataset}/{l_r}-{e_s}_{fair}.csv\", names=[\"Iteration\", \"User ID\", \"Item ID\", \"Predicted rating\", \"Loss\"])\n",
    "    loss_info[\"User ID\"] = loss_info[\"User ID\"].map(remappings[\"user_id\"])\n",
    "    loss_info[\"Item ID\"] = loss_info[\"Item ID\"].map(remappings[\"item_id\"])\n",
    "\n",
    "    return loss_info\n",
    "\n",
    "# loss_info = get_remapped_losses(\"bpr\", \"ml-1m\", \"0001\", \"128\")\n",
    "# loss_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to plot losses of different user groups\n",
    "def plot_losses(model, dataset, l_r, e_s, fair, sensitive_attributes):\n",
    "    plot_path = f\"training_losses/{model}/{dataset}/{l_r}-{e_s}_{fair}_user.png\"\n",
    "    if os.path.exists(plot_path):\n",
    "        display.display(Image.open(plot_path))\n",
    "        return\n",
    "    \n",
    "    loss_info = get_remapped_losses(model, dataset, l_r, e_s, fair)\n",
    "    item_info = pd.read_csv(f\"{dataset}/{dataset}.item\", sep=\"\\t\", engine=\"python\", encoding=\"latin-1\", index_col=0, header=0)\n",
    "    user_info = pd.read_csv(f\"{dataset}/{dataset}.user\", sep=\"\\t\", engine=\"python\", index_col=0, header=0)\n",
    "        \n",
    "    iterations = set(loss_info[\"Iteration\"])\n",
    "    groups_info = {}\n",
    "    \n",
    "    if sensitive_attributes[\"user\"]:\n",
    "        if sensitive_attributes[\"item\"]:\n",
    "            split = \"both\"\n",
    "        else:\n",
    "            split = \"user\"\n",
    "    else:\n",
    "        split = \"item\"\n",
    "\n",
    "    if split == \"both\":\n",
    "        for u_group in set(user_info[sensitive_attributes[\"user\"]]):\n",
    "            if u_group == None:\n",
    "                continue\n",
    "            groups_info[u_group] = {}\n",
    "            for i_group in set(item_info[sensitive_attributes[\"item\"]]):\n",
    "                if i_group == None:\n",
    "                    continue\n",
    "                groups_info[u_group][i_group] = {\"indices\": [], \"losses\": []}\n",
    "                groups_info[u_group][i_group][\"indices\"].append(list(user_info[user_info[sensitive_attributes[\"user\"]] == u_group].index))\n",
    "                groups_info[u_group][i_group][\"indices\"].append(list(item_info[item_info[sensitive_attributes[\"item\"]] == i_group].index))\n",
    "    elif split == \"user\":\n",
    "        for group in set(user_info[sensitive_attributes[\"user\"]]):\n",
    "            if group == None:\n",
    "                continue\n",
    "            groups_info[group] = {\"losses\": []}\n",
    "            try:\n",
    "                groups_info[group][\"indices\"] = [int(i) for i in list(user_info[user_info[sensitive_attributes[\"user\"]] == group].index)]\n",
    "            except ValueError:\n",
    "                groups_info[group][\"indices\"] = [i for i in list(user_info[user_info[sensitive_attributes[\"user\"]] == group].index)]\n",
    "\n",
    "    elif split == \"item\":\n",
    "        for group in set(item_info[sensitive_attributes[\"item\"]]):\n",
    "            if group == None:\n",
    "                continue\n",
    "            groups_info[group] = {\"losses\": []}\n",
    "            groups_info[group][\"indices\"] = list(item_info[item_info[sensitive_attributes[\"item\"]] == group].index)\n",
    "\n",
    "    for i in iterations:\n",
    "        iteration_loss_info = loss_info[loss_info[\"Iteration\"] == i]\n",
    "        if split == \"both\":\n",
    "            for u_group in groups_info.keys():\n",
    "                for i_group in groups_info[u_group].keys():\n",
    "                    u_indices, i_indices = groups_info[u_group][i_group][\"indices\"]\n",
    "                    group_loss_info = iteration_loss_info[iteration_loss_info[\"User ID\"].isin(u_indices)]\n",
    "                    group_loss_info = group_loss_info[group_loss_info[\"Item ID\"].isin(i_indices)]\n",
    "                    groups_info[u_group][i_group][\"losses\"].append(group_loss_info[\"Loss\"])\n",
    "        else:\n",
    "            for group in groups_info.keys():\n",
    "                if split == \"user\":\n",
    "                    group_loss_info = iteration_loss_info[iteration_loss_info[\"User ID\"].isin(groups_info[group][\"indices\"])]\n",
    "                elif split == \"item\":\n",
    "                    group_loss_info = iteration_loss_info[iteration_loss_info[\"Item ID\"].isin(groups_info[group][\"indices\"])]\n",
    "                groups_info[group][\"losses\"].append(group_loss_info[\"Loss\"])\n",
    "\n",
    "    if split == \"both\":\n",
    "        for u_group in groups_info.keys():\n",
    "            for i_group in groups_info[u_group].keys():\n",
    "                N = len(groups_info[u_group][i_group][\"losses\"][0])\n",
    "                plt.plot(list(iterations)[:-11], [losses.mean() for losses in groups_info[u_group][i_group][\"losses\"]][:-11], label=f\"User: {str(u_group)}, Item: {str(i_group)} (N = {N})\")\n",
    "    else:\n",
    "        all_group_losses = {}\n",
    "\n",
    "        final_losses = []\n",
    "\n",
    "        for group in groups_info.keys():\n",
    "            group_losses = [losses.mean() for losses in groups_info[group][\"losses\"]][:-11]\n",
    "            num_epochs = len(group_losses)\n",
    "            all_group_losses[group] = list(zip(range(num_epochs), group_losses, [group] * num_epochs))\n",
    "\n",
    "            N = len(groups_info[group][\"losses\"][0])\n",
    "            plt.plot(list(iterations)[:-11], [losses.mean() for losses in groups_info[group][\"losses\"]][:-11], label=f\"{group} (N = {N})\")\n",
    "            final_losses.append([losses.mean() for losses in groups_info[group][\"losses\"]][:-11][-1])\n",
    "\n",
    "        print(f\"user: {max(final_losses) / min(final_losses)}\")\n",
    "\n",
    "    # all_losses = []\n",
    "    # for i in range(num_epochs):\n",
    "    #     all_losses.append(all_group_losses[2][i])\n",
    "    #     all_losses.append(all_group_losses[1][i])\n",
    "    #     all_losses.append(all_group_losses[0][i])\n",
    "    # print(all_losses)\n",
    "    # with open(\"user_losses.csv\", \"a\") as f:\n",
    "    #     writer = csv.writer(f)\n",
    "    #     writer.writerows(all_losses)\n",
    "\n",
    "    plt.title(f\"{model.upper()} Loss over time per user group (lambda={fair[:-2]}.{fair[-2:]})\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(f\"{model.upper()} Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to plot losses of different item groups\n",
    "def plot_item_losses(model, dataset, l_r, e_s, fair):\n",
    "    colour_map = {\"H\": \"r\", \"M\": \"g\", \"T\": \"b\"}\n",
    "    item_groups = {\"H\": \"Head\", \"M\": \"Mid\", \"T\": \"Tail\"}\n",
    "    plot_path = f\"training_losses/{model}/{dataset}/{l_r}-{e_s}_{fair}_item.png\"\n",
    "\n",
    "    if os.path.exists(plot_path):\n",
    "        display.display(Image.open(plot_path))\n",
    "        return\n",
    "    \n",
    "    loss_info = get_remapped_losses(model, dataset, l_r, e_s, fair)\n",
    "    item_info = pd.read_csv(f\"{dataset}/{dataset}.item\", sep=\"\\t\", engine=\"python\", encoding=\"latin-1\", index_col=0, header=0)\n",
    "    \n",
    "    item_labels = [\"H\", \"M\", \"T\"]\n",
    "    item_indices = {item_label: item_info.loc[item_info[\"popular item\"] == item_label].index for item_label in item_labels}\n",
    "    grouped_item_losses = {item_label: [] for item_label in item_labels}\n",
    "    N_values = {}\n",
    "    iterations = sorted(set(loss_info[\"Iteration\"].values))\n",
    "    \n",
    "    for i in iterations:\n",
    "        iteration_loss_info = loss_info.loc[loss_info[\"Iteration\"] == i]\n",
    "        for item_label in item_labels:\n",
    "            iteration_losses = iteration_loss_info.loc[iteration_loss_info[\"Item ID\"].astype(float).isin(item_indices[item_label].astype(float))][\"Loss\"]\n",
    "            grouped_item_losses[item_label].append(iteration_losses.mean())\n",
    "            if i == 0:\n",
    "                N_values[item_label] = len(iteration_losses)\n",
    "\n",
    "\n",
    "    final_losses = [grouped_item_losses[item_label][-11] for item_label in item_labels]\n",
    "    if fair != \"base\":\n",
    "        print(f\"item: {max(final_losses) / min(final_losses)}\")\n",
    "    else:\n",
    "        print(f\"item (base): {max(final_losses) / min(final_losses)}\")\n",
    "\n",
    "    for item_label in item_labels:\n",
    "        label = f\"{item_groups[item_label]}\"\n",
    "        if fair == \"base\":\n",
    "            plt.plot(iterations[:-11], grouped_item_losses[item_label][:-11], label=f\"{label} (BPR)\", color=colour_map[item_label])\n",
    "        else:\n",
    "            plt.plot(iterations[:-11], grouped_item_losses[item_label][:-11], label=f\"{label} (ILE)\", color=colour_map[item_label], linestyle=\"dashed\")\n",
    "\n",
    "    if \"base\" not in plot_path:\n",
    "        plt.xlabel(\"Epoch\", fontsize=18)\n",
    "        plt.ylabel(\"Average Loss\", fontsize=18)\n",
    "        plot_item_losses(model, dataset, l_r, e_s, \"base\")\n",
    "\n",
    "    if fair == \"base\":\n",
    "        plt.legend(fontsize=12, loc=\"upper right\", ncols=2)\n",
    "        plt.xlabel(\"Epoch\", fontsize=18)\n",
    "        plt.ylabel(\"Average loss\", fontsize=18)\n",
    "        plt.savefig(plot_path)\n",
    "    \n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair = \"item020std\"\n",
    "\n",
    "# plot_losses(\"bpr\", \"goodreads\", \"0001\", \"128\", fair, {\"user\": \"mainstream class (even groups)\", \"item\": None})\n",
    "plot_item_losses(\"bpr\", \"goodreads\", \"0001\", \"128\", fair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPR - ML-1M - 0001-128 (DeepSVDD labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MS_similarity = np.load('../MS_DeepSVDD.npy')\n",
    "\n",
    "sorted_MS_scores = sorted(list(enumerate(MS_similarity)), key=lambda x: x[1])\n",
    "a = [t[0]+1 for t in sorted_MS_scores]\n",
    "b = sorted([list(range(5)) * 1208][0])\n",
    "c = dict(zip(a, b))\n",
    "user_info[\"test\"] = c\n",
    "\n",
    "plot_losses(loss_info, user_info, item_info, {\"user\": \"test\", \"item\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainstream_1 = sample(list(user_info[user_info[\"mainstream class (thresholds)\"] == 4].index), 1)\n",
    "mainstream_5 = sample(list(user_info[user_info[\"mainstream class (thresholds)\"] == 4].index), 5)\n",
    "niche_1 = sample(list(user_info[user_info[\"mainstream class (thresholds)\"] == 0].index), 1)\n",
    "niche_5 = sample(list(user_info[user_info[\"mainstream class (thresholds)\"] == 0].index), 5)\n",
    "\n",
    "sample_1, sample_5 = mainstream_1 + niche_1, mainstream_5 + niche_5\n",
    "\n",
    "plot_losses(loss_info[loss_info[\"User ID\"].isin(sample_1)], user_info, item_info, {\"user\": \"mainstream class (thresholds)\", \"item\": None})\n",
    "plot_losses(loss_info[loss_info[\"User ID\"].isin(sample_5)], user_info, item_info, {\"user\": \"mainstream class (thresholds)\", \"item\": None})\n",
    "\n",
    "mainstream_1 = sample(list(user_info[user_info[\"mainstream class (even groups)\"] == 4].index), 1)\n",
    "mainstream_5 = sample(list(user_info[user_info[\"mainstream class (even groups)\"] == 4].index), 5)\n",
    "niche_1 = sample(list(user_info[user_info[\"mainstream class (even groups)\"] == 0].index), 1)\n",
    "niche_5 = sample(list(user_info[user_info[\"mainstream class (even groups)\"] == 0].index), 5)\n",
    "\n",
    "sample_1, sample_5 = mainstream_1 + niche_1, mainstream_5 + niche_5\n",
    "\n",
    "plot_losses(loss_info[loss_info[\"User ID\"].isin(sample_1)], user_info, item_info, {\"user\": \"mainstream class (even groups)\", \"item\": None})\n",
    "plot_losses(loss_info[loss_info[\"User ID\"].isin(sample_5)], user_info, item_info, {\"user\": \"mainstream class (even groups)\", \"item\": None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_item_popularities(datasets):\n",
    "    for ds in datasets:\n",
    "        ds_items = pd.read_csv(f\"{ds}/{ds}.item\", sep=\"\\t\", engine=\"python\", encoding=\"latin-1\", index_col=0, header=0)\n",
    "        ds_inters = pd.read_csv(f\"{ds}/{ds}.inter\", sep=\"\\t\", engine=\"python\", encoding=\"latin-1\", header=0)\n",
    "\n",
    "        # Stores all given ratings for each item\n",
    "        all_item_ratings = {}\n",
    "        for item_idx in ds_items.index:\n",
    "            all_item_ratings[item_idx] = list(ds_inters[ds_inters[\"item_id:token\"] == item_idx][\"rating:float\"])\n",
    "\n",
    "        # Percentage of all ratings that belong to each item\n",
    "        ds_items[\"total ratings (%)\"] = {item_idx: len(all_item_ratings[item_idx]) / len(ds_inters.index) for item_idx in all_item_ratings.keys()}\n",
    "\n",
    "\n",
    "        plt.plot(range(len(ds_items.index)), sorted(ds_items[\"total ratings (%)\"], reverse=True), label=ds)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_item_popularities([\"ml-1m\", \"goodreads\", \"google_reviews\", \"yelp\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecBole-tG4M4pA8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
